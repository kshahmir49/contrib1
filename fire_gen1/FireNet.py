import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import math
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple, deque
from itertools import count
from torchsummary import summary
from torch.utils.data import TensorDataset, DataLoader
from torch.optim import Adam
from math import log10, sqrt
device = "cpu"

class Block(nn.Module):
    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):
        super().__init__()
        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)
        if up:

            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)
            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)
        else:
            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)
            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)
        self.bnorm1 = nn.BatchNorm2d(out_ch)
        self.bnorm2 = nn.BatchNorm2d(out_ch)
        self.relu  = nn.ReLU()
        
    def forward(self, x, t, ):
        # First Conv
        h = self.bnorm1(self.relu(self.conv1(x)))
        # Time embedding
        time_emb = self.relu(self.time_mlp(t))
        # Extend last 2 dimensions
        time_emb = time_emb[(..., ) + (None, ) * 2]
        # Add time channel
        h = h + time_emb
        # Second Conv
        h = self.bnorm2(self.relu(self.conv2(h)))
        # Down or Upsample
        return self.transform(h)


class SinusoidalPositionEmbeddings(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        # TODO: Double check the ordering here
        return embeddings


class SimpleUnet(nn.Module):
    """
    A simplified variant of the Unet architecture.
    """
    def __init__(self):
        super().__init__()
        image_channels = 1
        down_channels = (16, 32, 64)
        up_channels = (64, 32, 16)
        out_dim = 1
        time_emb_dim = 16

        # Time embedding
        self.time_mlp = nn.Sequential(
                SinusoidalPositionEmbeddings(time_emb_dim),
                nn.Linear(time_emb_dim, time_emb_dim),
                nn.ReLU()
            )
        
        # Initial projection
        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)

        # Downsample
        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels)-1)])
        # Upsample
        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels)-1)])
        
        # Edit: Corrected a bug found by Jakub C (see YouTube comment)
        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)

    def forward(self, x, timestep):
        # Embedd time
        t = self.time_mlp(timestep)
        # Initial conv
        x = self.conv0(x)
        # Unet
        residual_inputs = []
        for down in self.downs:
            x = down(x, t)
            residual_inputs.append(x)
        for up in self.ups:
            residual_x = residual_inputs.pop()
            # Add residual x as additional channels
            x = torch.cat((x, residual_x), dim=1)

            x = up(x, t)
        return self.output(x)

model = SimpleUnet()
# summary(model, [(1,100, 100), (1,1)])


def get_loss(model, x, y, t):
    y_pred = model(x, t)
    return F.l1_loss(y, y_pred)     




input = np.load("input_wofire.npy")
output = np.load("output_wofire.npy")
sensor_values = np.load("sensor_values_wofire.npy")

input = torch.from_numpy(input)
output = torch.from_numpy(output)
sensor_values = torch.from_numpy(sensor_values)
# last column
time = sensor_values[:,-1]
input, output, time = input.type(torch.float32), output.type(torch.float32), time.type(torch.float32)


input = input.resize_(input.shape[0], 1, input.shape[1], input.shape[2])
output = output.resize_(output.shape[0], 1, output.shape[1], output.shape[2])
# time = time.resize_(time.shape[0], 1)

model = model.to(device)
input = input.to(device)
output = output.to(device)
time = time.to(device)
train_tensor = TensorDataset(input, time, output)
train_dataloader = DataLoader(train_tensor, batch_size=50)
optimizer = Adam(model.parameters(), lr=0.001)
epochs = 2000

for epoch in range(epochs):
    for step, batch in enumerate(train_dataloader):
      optimizer.zero_grad()
      t = batch[1]
      loss = get_loss(model, batch[0], batch[2], t)
      loss.backward()
      optimizer.step()

      if epoch % 5 == 0 and step == 0:
        print(f"Epoch {epoch} | Loss: {loss.item()} ")

torch.save(model.state_dict(), "dm_unet_10K_wofire")